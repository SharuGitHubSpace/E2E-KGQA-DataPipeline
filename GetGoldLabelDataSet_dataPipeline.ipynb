{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SharuGitHubSpace/E2E-KGQA-DataPipeline/blob/main/GetGoldLabelDataSet_dataPipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdcUg6sn51E9",
        "outputId": "c2b8db9f-0da4-4121-dd20-86ba315124eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.0.9-py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SPARQLWrapper\n",
            "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
            "Collecting rdflib>=6.1.1\n",
            "  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate<0.7.0,>=0.6.0\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->SPARQLWrapper) (1.16.0)\n",
            "Installing collected packages: isodate, rdflib, SPARQLWrapper\n",
            "Successfully installed SPARQLWrapper-2.0.0 isodate-0.6.1 rdflib-6.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter\n",
        "!pip install SPARQLWrapper "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar3Z-ROF_b6t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json \n",
        "from urllib.request import urlopen "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT0iiBrkjrsI"
      },
      "outputs": [],
      "source": [
        "# parameter for urlopen\n",
        "url_train = r\"https://raw.githubusercontent.com/amazon-science/mintaka/main/data/mintaka_train.json\"\n",
        "url_dev = r\"https://raw.githubusercontent.com/amazon-science/mintaka/main/data/mintaka_dev.json\"\n",
        "url_test = r\"https://raw.githubusercontent.com/amazon-science/mintaka/main/data/mintaka_test.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tOP_z9jdUU3"
      },
      "outputs": [],
      "source": [
        "# callSparql('Q20762698','sing',False)\n",
        "def callSparql(Q_ID,A_ID,isAnswerEntity):\n",
        "      import sys\n",
        "      from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "      s_p_o_List = []\n",
        "      triple = []\n",
        "      Q_ID = Q_ID.replace('Q','wd:Q')\n",
        "      if isAnswerEntity:\n",
        "           A_ID = A_ID.replace('Q','wd:Q')\n",
        "          \n",
        "      else:\n",
        "           A_ID = A_ID.lower()\n",
        "           A_ID = '\\'' +  A_ID + '\\''\n",
        "\n",
        "\n",
        "      endpoint_url = \"https://query.wikidata.org/sparql\"\n",
        "      query = \"\"\"\n",
        "      PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "      PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
        "\n",
        "      SELECT distinct $s $p $o WHERE\n",
        "      { \n",
        "      {\n",
        "        VALUES ?s {  \"\"\" + Q_ID + \"\"\" } .\n",
        "        FILTER (lcase(str(?o)) IN (\"\"\"+ A_ID +\"\"\") )\n",
        "        FILTER(STRSTARTS(STR(?p), \"http://www.wikidata.org/prop/direct/\"))\n",
        "        FILTER (lang(?o) = \"en\")\n",
        "        $s $p $o\n",
        "      }\n",
        "      UNION\n",
        "      {\n",
        "          VALUES ?s { \"\"\" + Q_ID + \"\"\" } .    \n",
        "          VALUES ?o { \"\"\" + A_ID + \"\"\" } .\n",
        "        FILTER(STRSTARTS(STR(?p), \"http://www.wikidata.org/prop/direct/\"))\n",
        "        BIND( REPLACE( ?string,\"prop/direct/\",\"entity/\" ) AS ?entity ).\n",
        "        $s $p $o\n",
        "      }\n",
        "      }\n",
        "      ORDER BY DESC(?s)\n",
        "      \"\"\"\n",
        "\n",
        "      results = get_results(endpoint_url, query)\n",
        "      \n",
        "      for result in results[\"results\"][\"bindings\"]:\n",
        "          #print(result)\n",
        "          #print(result['s']['value'])\n",
        "          #print(result['p']['value'])\n",
        "          #print(result['o']['value'])\n",
        "          s = (result['s']['value']).replace('http://www.wikidata.org/entity/', '')\n",
        "          p = (result['p']['value']).replace('http://www.wikidata.org/prop/direct/', '')\n",
        "          o = (result['o']['value']).replace('http://www.wikidata.org/entity/', '')\n",
        "          triple.append((s , p , o ))\n",
        "          #print(triple)\n",
        "          #s_p_o_List.append(triple)  \n",
        "      return triple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDi5oxoVfNaV"
      },
      "outputs": [],
      "source": [
        "def get_results(endpoint_url, query):\n",
        "      import sys\n",
        "      from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "      #print(query)\n",
        "      user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
        "      # TODO adjust user agent; see https://w.wiki/CX6\n",
        "      sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
        "      sparql.setQuery(query)\n",
        "      sparql.setReturnFormat(JSON)\n",
        "      return sparql.query().convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n02dyCEKeRMA"
      },
      "outputs": [],
      "source": [
        "# Steps to get the Goldendataset for Model training - Trial 4\n",
        "# Step 1 : Split the Train(1750)  - Val(250) - Test(500) \n",
        "          # conditions are category  = movies , keep questions only that are in answer entity type 'entity' and 'string'\n",
        "        ###  ###  ###  ###  ###  ###  LOOP             ###  ###  ###  ###\n",
        "# Step 2 : Follow steps to get the DS for Set 1 \n",
        "           # loop every question \n",
        "           # Extract all QID from question \n",
        "           # Extract all Answer id from question\n",
        "           # write to a file QID-AID-Log.xslx (With reference to train / val / test) every qid - aid , if there are 2 qid make two rows Q_QID1 - A_QID1 ,            \n",
        "           # Log all combinations\n",
        "           \n",
        "# Step 3 : SPARQL - Find if the QID -AID pair has a  predicates from SPARQL\n",
        "           # if there is match , put the QID-AID pair and s-p-o in the triplet_csv.xslx\n",
        "           # Copy the QID1 and QID2 and AID to entity_csv.xslx\n",
        "           # copy the predicate to predicate_csv.xlsx\n",
        "           # Outcome of this step is the S -> P -> O triplets triplet_csv_master.xlsx\n",
        "\n",
        "# Step 4 : # From last step map literals to the unique QID and remove duplicates  call it literal_csv.csv\n",
        "\n",
        "# Step 5 : # Prepare the vlookup in the excel \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WbumumlhzYs"
      },
      "outputs": [],
      "source": [
        "# Train - Val - Test Split\n",
        "train_split = 875\n",
        "val_split = 125\n",
        "test_split = 250\n",
        "\n",
        "topics = [\"movies\",\"history\",\"sports\",\"geography\"]\n",
        "answer_filter_types = ['entity','string' , 'date']\n",
        "\n",
        "\n",
        "urls = [] \n",
        "urls.append(url_train)\n",
        "urls.append(url_dev)\n",
        "urls.append(url_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qhcr6sFN8GBN"
      },
      "outputs": [],
      "source": [
        "dataForDataSet_Train = []\n",
        "dataForDataSet_Val = []\n",
        "dataForDataSet_Test = []\n",
        "\n",
        "Question_Entity_id = []\n",
        "Answer_Entity_id = []\n",
        "# final\n",
        "s_p_o = []\n",
        "entities = []\n",
        "entities_Answer = []\n",
        "predicates = []\n",
        "Q_A_List = {}\n",
        "for url in urls:\n",
        "    response = urlopen(url)\n",
        "    data_json1 = json.loads(response.read())  \n",
        "    for i in range(len(data_json1)):\n",
        "      A_temp = \"\"\n",
        "      Q_temp = \"\"\n",
        "      triple = \"\"\n",
        "      predicate = \"\"\n",
        "      Q_temp_list = []\n",
        "      data_json = data_json1[i]\n",
        "      isAnswerEntity = False\n",
        "      isAnswerEntityMultiple = False\n",
        "      alltuplesFound = True\n",
        "      A_temp_list = []\n",
        "      if (data_json[\"category\"] in topics) and (data_json[\"answer\"][\"answerType\"] in answer_filter_types[0]): # Q_QID , A_QID -- Entities\n",
        "            # first , get the QID and AID to query SPARQL \n",
        "            if not (data_json[\"answer\"].get(\"answer\") is None) and not (data_json[\"answer\"].get(\"answer\") in ['null']): \n",
        "                  #print(data_json[\"answer\"].get(\"answer\"))      \n",
        "                  for k in range(len(data_json[\"answer\"][\"answer\"])):\n",
        "                    if k > 1:\n",
        "                        isAnswerEntityMultiple = True\n",
        "                    if not (data_json[\"answer\"][\"answer\"][k] is None):                         \n",
        "                          # loop for entities\n",
        "                          if 'name' in str(data_json[\"answer\"][\"answer\"][k]): \n",
        "                              if str(data_json[\"answer\"][\"answer\"][k][\"name\"]).startswith(\"Q\"):\n",
        "                                A_temp = data_json[\"answer\"][\"answer\"][k][\"name\"]\n",
        "                                A_temp_list.append(A_temp) \n",
        "                                isAnswerEntity = True\n",
        "                          else:  # literals                      \n",
        "                                A_temp = data_json[\"answer\"][\"answer\"][k]\n",
        "                    Answer_Entity_id.append(A_temp)   # Master list  \n",
        "\n",
        "                    for j in range(len(data_json['questionEntity'])): # get the question id only if there is an answer                               \n",
        "                          if str(data_json[\"questionEntity\"][j][\"name\"]).startswith(\"Q\"):\n",
        "                              Q_temp  = str(data_json[\"questionEntity\"][j][\"name\"])\n",
        "                              Q_temp_list.append(Q_temp)\n",
        "                              Question_Entity_id.append(Q_temp)  \n",
        "                              Q_A_List[Q_temp] = A_temp\n",
        "            # Now take the QID and AID and Query Sparkl see if there is P ID hit ; set a boolean value\n",
        "            # For every element in Q_temp_list with A_temp\n",
        "            # call SPARQL\n",
        "            for question_id in Q_temp_list:\n",
        "               #print(len(A_temp_list))\n",
        "               if len(A_temp_list) > 1 :                    \n",
        "                    for a_qid in A_temp_list:\n",
        "                        if alltuplesFound:\n",
        "                          triple = callSparql(question_id, a_qid,isAnswerEntity)\n",
        "                          if not triple:\n",
        "                              alltuplesFound = False\n",
        "                              data_json = ''\n",
        "                          else : \n",
        "                              alltuplesFound = True\n",
        "                              #predicate =  triple[0][1]                \n",
        "                              # if there is a predicate then we are going to \n",
        "                              # Put the S - P - O in the list\n",
        "                              s_p_o.append(triple)\n",
        "                              # Put the Question Entity id in the Entity List entity_csv\n",
        "                              entities.append(question_id)\n",
        "                              # Put the Answer in the Answer Entity List \n",
        "                              if isAnswerEntity:\n",
        "                                entities.append(a_qid)\n",
        "                              else:\n",
        "                                entities_Answer.append(a_qid)\n",
        "                              # Put the predicate in the Predicate Entity List predicate_csv\n",
        "                              for t in triple:                                    \n",
        "                                 predicate =  t[1]  \n",
        "                                 predicates.append(predicate)\n",
        "               else:                      \n",
        "                      triple = callSparql(question_id, A_temp,isAnswerEntity)\n",
        "                      #print(triple)\n",
        "                      #print(data_json)\n",
        "                      if not triple:\n",
        "                          alltuplesFound = False\n",
        "                          pass\n",
        "                      else :                          \n",
        "                          alltuplesFound = True                                        \n",
        "                          # if there is a predicate then we are going to \n",
        "                          # Put the S - P - O in the list                          \n",
        "                          s_p_o.append(triple)\n",
        "                          # Put the Question Entity id in the Entity List entity_csv\n",
        "                          entities.append(question_id)\n",
        "                          # Put the Answer in the Answer Entity List \n",
        "                          if isAnswerEntity:\n",
        "                            entities.append(A_temp)\n",
        "                          else:\n",
        "                            entities_Answer.append(A_temp)\n",
        "                          # Put the predicate in the Predicate Entity List predicate_csv\n",
        "                          for t in triple:                            \n",
        "                            predicate =  t[1]  \n",
        "                            predicates.append(predicate)\n",
        "                      # final step - take only the question set which has the triple in Wikidata and put in the Master DS\n",
        "     \n",
        "            if alltuplesFound:\n",
        "                if urls.index(url) == 0 :\n",
        "                  dataForDataSet_Train.append(data_json)   \n",
        "                elif urls.index(url) == 1 :\n",
        "                  dataForDataSet_Val.append(data_json)    \n",
        "                elif urls.index(url) == 2 :\n",
        "                  dataForDataSet_Test.append(data_json)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pnnY31ICokB",
        "outputId": "7e3469a8-88a9-478f-e35f-6782bb0ef377"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "282"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(dataForDataSet_Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEl50ot-5ISp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def callSparql_getPredicate(predicateList):\n",
        "      import sys\n",
        "      from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "      import pandas as pd\n",
        "      import openpyxl\n",
        "      predicateList =  list(set(predicateList))\n",
        "      predicateList =  [w.replace('P', 'wdt:P') for w in predicateList]\n",
        "      PID_SPARQL_QUERY = ' '.join(str(x) for x in predicateList) \n",
        "      predicate = []\n",
        "      endpoint_url = \"https://query.wikidata.org/sparql\"\n",
        "      query = \"\"\"\n",
        "      SELECT distinct ?wdt ?wdLabel ?wdAltLabel\n",
        "            {\n",
        "            VALUES ?wdt { \"\"\" + PID_SPARQL_QUERY + \"\"\" }\n",
        "            ?wd wikibase:directClaim ?wdt .\n",
        "            SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "            }\n",
        "      \"\"\"\n",
        "      print(query)\n",
        "      results = get_results(endpoint_url, query)\n",
        "           \n",
        "      for result in results[\"results\"][\"bindings\"]:\n",
        "          #print(result)      \n",
        "          p = (result['wdt']['value']).replace('http://www.wikidata.org/prop/direct/', '')    \n",
        "          p_label = (result['wdLabel']['value'])  \n",
        "                               \n",
        "          if  'wdAltLabel' in result:  \n",
        "             p_alias =  (result['wdAltLabel']['value'])     \n",
        "             predicate.append((p , p_label , p_alias ))  \n",
        "          else:\n",
        "             predicate.append((p , p_label , '' ))         \n",
        "      return predicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA9LMwWwweG9"
      },
      "outputs": [],
      "source": [
        "# Make datasets\n",
        "with open('train_Movies_history_sports_geo_v1_01042023.txt','w', encoding='utf-8') as f:\n",
        "  for line in dataForDataSet_Train:\n",
        "    json.dump(line, f, ensure_ascii=False)\n",
        "    f.write('\\n')\n",
        "with open('dev_movies_history_sports_geo_v1_01042023.txt','w', encoding='utf-8') as f:\n",
        "  for line in dataForDataSet_Val:\n",
        "    json.dump(line, f, ensure_ascii=False)\n",
        "    f.write('\\n')\n",
        "with open( 'test_movies_history_sports_geo_v1_01042023.txt','w', encoding='utf-8') as f:\n",
        "  for line in dataForDataSet_Test:\n",
        "    json.dump(line, f, ensure_ascii=False)\n",
        "    f.write('\\n')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLx_3uH67uyV",
        "outputId": "8eec7289-1155-47ff-9811-6bea231bb184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      SELECT distinct ?wdt ?wdLabel ?wdAltLabel\n",
            "            {\n",
            "            VALUES ?wdt { wdt:P1889 wdt:P1344 wdt:P156 wdt:P1923 wdt:P20 wdt:P3342 wdt:P115 wdt:P3018 wdt:P463 wdt:P3448 wdt:P27 wdt:P144 wdt:P2341 wdt:P1412 wdt:P451 wdt:P36 wdt:P50 wdt:P725 wdt:P793 wdt:P1029 wdt:P166 wdt:P58 wdt:P102 wdt:P527 wdt:P1411 wdt:P4584 wdt:P210 wdt:P413 wdt:P734 wdt:P530 wdt:P276 wdt:P57 wdt:P735 wdt:P488 wdt:P157 wdt:P1336 wdt:P17 wdt:P6166 wdt:P140 wdt:P186 wdt:P1441 wdt:P6 wdt:P86 wdt:P162 wdt:P495 wdt:P1891 wdt:P361 wdt:P825 wdt:P1444 wdt:P159 wdt:P610 wdt:P466 wdt:P7047 wdt:P1445 wdt:P828 wdt:P1346 wdt:P180 wdt:P1830 wdt:P710 wdt:P1268 wdt:P1165 wdt:P5869 wdt:P840 wdt:P664 wdt:P634 wdt:P287 wdt:P737 wdt:P205 wdt:P937 wdt:P35 wdt:P155 wdt:P674 wdt:P1327 wdt:P647 wdt:P61 wdt:P4791 wdt:P1431 wdt:P921 wdt:P1434 wdt:P800 wdt:P25 wdt:P179 wdt:P726 wdt:P22 wdt:P170 wdt:P175 wdt:P26 wdt:P190 wdt:P3373 wdt:P802 wdt:P631 wdt:P112 wdt:P1366 wdt:P69 wdt:P272 wdt:P559 wdt:P706 wdt:P1376 wdt:P54 wdt:P31 wdt:P3279 wdt:P241 wdt:P641 wdt:P138 wdt:P974 wdt:P344 wdt:P607 wdt:P1308 wdt:P2596 wdt:P88 wdt:P30 wdt:P150 wdt:P21 wdt:P286 wdt:P1589 wdt:P10606 wdt:P1066 wdt:P3173 wdt:P40 wdt:P1542 wdt:P127 wdt:P822 wdt:P676 wdt:P47 wdt:P1040 wdt:P6193 wdt:P551 wdt:P131 wdt:P206 wdt:P4969 wdt:P137 wdt:P200 wdt:P161 wdt:P1038 wdt:P84 wdt:P19 }\n",
            "            ?wd wikibase:directClaim ?wdt .\n",
            "            SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
            "            }\n",
            "      \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openpyxl\n",
        "list1 = callSparql_getPredicate(predicates)\n",
        "list1\n",
        "df = pd.DataFrame( list1 , columns=['p_id', 'p_label', 'p_alias']) \n",
        "df.to_excel('predicate_csv.xlsx', sheet_name='new_sheet_name')    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6m6LFtWDMuJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def callSparql_getEntities(entityList):\n",
        "      import sys\n",
        "      from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "      import pandas as pd\n",
        "      import openpyxl\n",
        "\n",
        "      entityList =  list(set(entityList))\n",
        "      entityList =  [w.replace('Q', 'wd:Q') for w in entityList]\n",
        "      #QID_SPARQL_QUERY = ' '.join(str(x) for x in entityList) \n",
        "      entity = []\n",
        "      endpoint_url = \"https://query.wikidata.org/sparql\"\n",
        "      \n",
        "      for q_id in entityList:\n",
        "          \n",
        "          query = \"\"\"\n",
        "                SELECT (?property as ?entity_id) \n",
        "                ?propertyLabel \n",
        "                  (GROUP_CONCAT(DISTINCT(?altLabel); separator = \", \") AS ?entity_alias) WHERE {\n",
        "                  VALUES ?property {\"\"\" + q_id + \"\"\"} .     #Concat all the Entity id's here\n",
        "                  OPTIONAL { ?property skos:altLabel ?altLabel . FILTER (lang(?altLabel) = \"en\") }\n",
        "                  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" .}\n",
        "                }\n",
        "                GROUP BY ?property ?propertyLabel ?propertyDescription\n",
        "          \"\"\"\n",
        "          #print(query)\n",
        "          results = get_results(endpoint_url, query)\n",
        "              \n",
        "          for result in results[\"results\"][\"bindings\"]:\n",
        "              #print(result)      \n",
        "              e = (result['entity_id']['value']).replace('http://www.wikidata.org/entity/', '')    \n",
        "              e_label = (result['propertyLabel']['value'])  \n",
        "                                  \n",
        "              if  'entity_alias' in result:  \n",
        "                e_alias =  (result['entity_alias']['value'])     \n",
        "                entity.append((e , e_label , e_alias ))  \n",
        "              else:\n",
        "                entity.append((e , e_label , '' ))       \n",
        "\n",
        "      return entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3PogGXFGn2J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import openpyxl\n",
        "list1 = callSparql_getEntities(entities)\n",
        "list1\n",
        "df = pd.DataFrame( list1 , columns=['e_id', 'e_label', 'e_alias']) \n",
        "df.to_excel('entity_csv.xlsx', sheet_name='entities')    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AgI-NulMdUJ"
      },
      "outputs": [],
      "source": [
        "# Convert all lists to excel \n",
        "\n",
        "df3 = pd.DataFrame(entities_Answer)\n",
        "writer = pd.ExcelWriter( 'entitiesLiterals.xlsx', engine='xlsxwriter')\n",
        "df3.to_excel(writer, sheet_name='AnswerLiteral', index=False)\n",
        "writer.save()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vkpL5qHQOYg",
        "outputId": "2b538148-9426-47c9-d29e-7526bf29603e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "entities_Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3NCZhimOBcs"
      },
      "outputs": [],
      "source": [
        "# Triplet List csv\n",
        "spo_list = []\n",
        "for spo in s_p_o:   \n",
        "  for x in spo:\n",
        "    spo_list.append(x)\n",
        "spo_list\n",
        "\n",
        "df2 = pd.DataFrame(spo_list , columns=['s_id', 'p_id', 'o_id']) \n",
        "writer = pd.ExcelWriter( 'triplet_csv.xlsx', engine='xlsxwriter')\n",
        "df2.to_excel(writer, sheet_name='triples', index=False)\n",
        "writer.save()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvhBshiZnCaI"
      },
      "outputs": [],
      "source": [
        "test = callSparql('Q20762698','sing1',False)\n",
        "if not test:\n",
        "  pass\n",
        "else :\n",
        " test[0][1]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}